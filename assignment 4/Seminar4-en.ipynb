{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano, Lasagne\n",
    "and why they matter\n",
    "\n",
    "\n",
    "### got no lasagne?\n",
    "Install the __bleeding edge__ version from here: http://lasagne.readthedocs.org/en/latest/user/installation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warming up\n",
    "* Implement a function that computes the sum of squares of numbers from 0 to N\n",
    "* Use numpy or python\n",
    "* An array of numbers 0 to N - numpy.arange(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sum_squares(N):\n",
    "    range = np.arange(N)\n",
    "    return np.sum(np.multiply(range, range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 505 ms, sys: 572 ms, total: 1.08 s\n",
      "Wall time: 1.08 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "662921401752298880"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sum_squares(10**8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# theano teaser\n",
    "\n",
    "Doing the very same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#I gonna be function parameter\n",
    "N = T.scalar(\"a dimension\",dtype='int32')\n",
    "\n",
    "\n",
    "#i am a recipe on how to produce sum of squares of arange of N given N\n",
    "result = (T.arange(N)**2).sum()\n",
    "\n",
    "#Compiling the recipe of computing \"result\" given N\n",
    "sum_function = theano.function(inputs = [N],outputs=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 442 ms, sys: 123 ms, total: 565 ms\n",
      "Wall time: 572 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(662921401752298880)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sum_function(10**8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does it work?\n",
    "* 1 You define inputs f your future function;\n",
    "* 2 You write a recipe for some transformation of inputs;\n",
    "* 3 You compile it;\n",
    "* You have just got a function!\n",
    "* The gobbledegooky version: you define a function as symbolic computation graph.\n",
    "\n",
    "\n",
    "* There are two main kinвs of entities: \"Inputs\" and \"Transformations\"\n",
    "* Both can be numbers, vectors, matrices, tensors, etc.\n",
    "* Both can be integers, floats of booleans (uint8) of various size.\n",
    "\n",
    "\n",
    "* An input is a placeholder for function parameters.\n",
    " * N from example above\n",
    "\n",
    "\n",
    "* Transformations are the recipes for computing something given inputs and transformation\n",
    " * (T.arange(N)^2).sum() are 3 sequential transformations of N\n",
    " * Doubles all functions of numpy vector syntax\n",
    " * You can almost always go with replacing \"np.function\" with \"T.function\" aka \"theano.tensor.function\"\n",
    "   * np.mean -> T.mean\n",
    "   * np.arange -> T.arange\n",
    "   * np.cumsum -> T.cumsum\n",
    "   * and so on.\n",
    "   * builtin operations also work that way\n",
    "   * np.arange(10).mean() -> T.arange(10).mean()\n",
    "   * Once upon a blue moon the functions have different names or locations (e.g. T.extra_ops)\n",
    "     * Ask us or google it\n",
    " \n",
    " \n",
    "Still confused? We gonna fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Inputs\n",
    "example_input_integer = T.scalar(\"scalar input\",dtype='float32')\n",
    "\n",
    "example_input_tensor = T.tensor4(\"four dimensional tensor input\") #dtype = theano.config.floatX by default\n",
    "#не бойся, тензор нам не пригодится\n",
    "\n",
    "\n",
    "\n",
    "input_vector = T.vector(\"\", dtype='int32') # vector of integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transformations\n",
    "\n",
    "#transofrmation: elementwise multiplication\n",
    "double_the_vector = input_vector*2\n",
    "\n",
    "#elementwise cosine\n",
    "elementwise_cosine = T.cos(input_vector)\n",
    "\n",
    "#difference between squared vector and vector itself\n",
    "vector_squares = input_vector**2 - input_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Practice time:\n",
    "#create two vectors of size float32\n",
    "my_vector = T.vector(\"\", dtype='float32')\n",
    "my_vector2 = T.vector(\"\", dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Write a transformation(recipe):\n",
    "#(vec1)*(vec2) / (sin(vec1) +1)\n",
    "my_transformation = my_vector * my_vector2 / (T.sin(my_vector) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elemwise{true_div,no_inplace}.0\n"
     ]
    }
   ],
   "source": [
    "print my_transformation\n",
    "#it's okay it aint a number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling\n",
    "* So far we were using \"symbolic\" variables and transformations\n",
    " * Defining the recipe for computation, but not computing anything\n",
    "* To use the recipe, one should compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = [my_vector, my_vector2]\n",
    "outputs = [my_transformation]\n",
    "\n",
    "# The next lines compile a function that takes two vectors and computes your transformation\n",
    "my_function = theano.function(\n",
    "    inputs,outputs,\n",
    "    allow_input_downcast=True #automatic type casting for input parameters (e.g. float64 -> float32)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using python lists:\n",
      "[array([  2.1721766 ,   5.23752832,  15.77397728], dtype=float32)]\n",
      "\n",
      "using numpy arrays:\n",
      "[array([   0.        ,    2.77555895,    5.47030783,   14.02131271,\n",
      "         89.5477066 ,  676.25805664,   47.183918  ,   24.4084301 ,\n",
      "         23.68156242,   38.24041748], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#using function with, lists:\n",
    "print \"using python lists:\"\n",
    "print my_function([1,2,3],[4,5,6])\n",
    "print\n",
    "\n",
    "#Or using numpy arrays:\n",
    "#btw, that 'float' dtype is casted to secong parameter dtype which is float32\n",
    "print \"using numpy arrays:\"\n",
    "print my_function(np.arange(10),\n",
    "                  np.linspace(5,6,10,dtype='float'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging\n",
    "* Compilation can take a while for big functions\n",
    "* To avoid waiting, one can evaluate transformations without compiling\n",
    "* Without compilation, the code runs slower, so consider reducing input size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.1721766    5.23752832  15.77397728]\n",
      "add 2 vectors [ 5.  7.  9.]\n",
      "vector's shape: [3]\n"
     ]
    }
   ],
   "source": [
    "#a dictionary of inputs\n",
    "my_function_inputs = {\n",
    "    my_vector:[1,2,3],\n",
    "    my_vector2:[4,5,6]\n",
    "}\n",
    "\n",
    "# evaluate my_transformation\n",
    "# has to match with compiled function output\n",
    "print my_transformation.eval(my_function_inputs)\n",
    "\n",
    "\n",
    "# can compute transformations on the fly\n",
    "print \"add 2 vectors\", (my_vector + my_vector2).eval(my_function_inputs)\n",
    "\n",
    "#!WARNING! if your transformation only depends on some inputs,\n",
    "#do not provide the rest of them\n",
    "print \"vector's shape:\", my_vector.shape.eval({\n",
    "        my_vector:[1,2,3]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Для отладки желательно уменьшить масштаб задачи. Если вы планировали послать на вход вектор из 10^9 примеров, пошлите 10~100.\n",
    "* Если #ОЧЕНЬ нужно послать большой вектор, быстрее скомпилировать функцию обычным способом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теперь сам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Quest #1 - implement a function that computes a mean squared error of two input vectors\n",
    "# Your function has to take 2 vectors and return a single number\n",
    "\n",
    "vec1 = T.vector(\"\", dtype=\"float32\")\n",
    "vec2 = T.vector(\"\", dtype=\"float32\")\n",
    "mse = ((vec1 - vec2) ** 2).mean()\n",
    "\n",
    "compute_mse = theano.function(\n",
    "    [vec1, vec2],\n",
    "    [mse],\n",
    "    allow_input_downcast=True #automatic type casting for input parameters (e.g. float64 -> float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "for n in [1,5,10,10**3]:\n",
    "    \n",
    "    elems = [np.arange(n),np.arange(n,0,-1), np.zeros(n),\n",
    "             np.ones(n),np.random.random(n),np.random.randint(100,size=n)]\n",
    "    \n",
    "    for el in elems:\n",
    "        for el_2 in elems:\n",
    "            true_mse = np.array(mean_squared_error(el,el_2))\n",
    "            my_mse = compute_mse(el,el_2)\n",
    "            if not np.allclose(true_mse,my_mse):\n",
    "                print 'Wrong result:'\n",
    "                print 'mse(%s,%s)'%(el,el_2)\n",
    "                print \"should be: %f, but your function returned %f\"%(true_mse,my_mse)\n",
    "                raise ValueError,\"Что-то не так\"\n",
    "\n",
    "print \"All tests passed\"\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared variables\n",
    "\n",
    "* The inputs and transformations only exist when function is called\n",
    "\n",
    "* Shared variables always stay in memory like global variables\n",
    " * Shared variables can be included into a symbolic graph\n",
    " * They can be set and evaluated using special methods\n",
    "   * but they can't change value arbitrarily during symbolic graph computation\n",
    "   * we'll cover that later;\n",
    " \n",
    " \n",
    "* Hint: such variables are a perfect place to store network parameters\n",
    " * e.g. weights or some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating shared variable\n",
    "shared_vector_1 = theano.shared(np.ones(10,dtype='float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial value [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#evaluating shared variable (outside symbolicd graph)\n",
    "print \"initial value\",shared_vector_1.get_value()\n",
    "\n",
    "# within symbolic graph you use them just as any other inout or transformation, not \"get value\" needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new value [ 0.  1.  2.  3.  4.]\n"
     ]
    }
   ],
   "source": [
    "#setting new value\n",
    "shared_vector_1.set_value( np.arange(5) )\n",
    "\n",
    "#getting that new value\n",
    "print \"new value\", shared_vector_1.get_value()\n",
    "\n",
    "#Note that the vector changed shape\n",
    "#This is entirely allowed... unless your graph is hard-wired to work with some fixed shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a recipe (transformation) that computes an elementwise transformation of shared_vector and input_scalar\n",
    "#Compile as a function of input_scalar\n",
    "\n",
    "input_scalar = T.scalar('coefficient',dtype='float32')\n",
    "\n",
    "scalar_times_shared = input_scalar * shared_vector_1\n",
    "\n",
    "\n",
    "shared_times_n = theano.function(\n",
    "    [input_scalar],\n",
    "    [scalar_times_shared]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared: [ 0.  1.  2.  3.  4.]\n",
      "shared_times_n(5) [array([  0.,   5.,  10.,  15.,  20.])]\n",
      "shared_times_n(-0.5) [array([-0. , -0.5, -1. , -1.5, -2. ])]\n"
     ]
    }
   ],
   "source": [
    "print \"shared:\", shared_vector_1.get_value()\n",
    "\n",
    "print \"shared_times_n(5)\",shared_times_n(5)\n",
    "\n",
    "print \"shared_times_n(-0.5)\",shared_times_n(-0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared: [-1.  0.  1.]\n",
      "shared_times_n(5) [array([-5.,  0.,  5.])]\n",
      "shared_times_n(-0.5) [array([ 0.5, -0. , -0.5])]\n"
     ]
    }
   ],
   "source": [
    "#Changing value of vector 1 (output should change)\n",
    "shared_vector_1.set_value([-1,0,1])\n",
    "print \"shared:\", shared_vector_1.get_value()\n",
    "\n",
    "print \"shared_times_n(5)\",shared_times_n(5)\n",
    "\n",
    "print \"shared_times_n(-0.5)\",shared_times_n(-0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T.grad - why theano matters\n",
    "* Theano can compute derivatives and gradients automatically\n",
    "* Derivatives are computed symbolically, not numerically\n",
    "\n",
    "Limitations:\n",
    "* You can only compute a gradient of a __scalar__ transformation over one or several scalar or vector (or tensor) transformations or inputs.\n",
    "* A transformation has to have float32 or float64 dtype throughout the whole computation graph\n",
    " * derivative over an integer has no mathematical sense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_scalar = T.scalar(name='input',dtype='float64')\n",
    "\n",
    "scalar_squared = T.sum(my_scalar**2)\n",
    "\n",
    "#a derivative of v_squared by my_vector\n",
    "derivative = T.grad(scalar_squared,my_scalar)\n",
    "\n",
    "fun = theano.function([my_scalar],scalar_squared)\n",
    "grad = theano.function([my_scalar],derivative) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6146bfbcd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAEACAYAAABBDJb9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfyhY5ofu40F1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6150211ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "x = np.linspace(-3,3)\n",
    "x_squared = map(fun,x)\n",
    "x_squared_der = map(grad,x)\n",
    "\n",
    "plt.plot(x, x_squared,label=\"x^2\")\n",
    "plt.plot(x, x_squared_der, label=\"derivative\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why that rocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "my_vector = T.vector('float64')\n",
    "\n",
    "#Compute the gradient of the next weird function over my_scalar and my_vector\n",
    "#warning! Trying to understand the meaning of that function may result in permanent brain damage\n",
    "\n",
    "weird_psychotic_function = ((my_vector+my_scalar)**(1+T.var(my_vector)) +1./T.arcsinh(my_scalar)).mean()/(my_scalar**2 +1) + 0.01*T.sin(2*my_scalar**1.5)*(T.sum(my_vector)* my_scalar**2)*T.exp((my_scalar-4)**2)/(1+T.exp((my_scalar-4)**2))*(1.-(T.exp(-(my_scalar-4)**2))/(1+T.exp(-(my_scalar-4)**2)))**2\n",
    "\n",
    "\n",
    "der_by_scalar,der_by_vector = T.grad(weird_psychotic_function, [my_scalar, my_vector])\n",
    "\n",
    "\n",
    "compute_weird_function = theano.function([my_scalar,my_vector],weird_psychotic_function)\n",
    "compute_der_by_scalar = theano.function([my_scalar,my_vector],der_by_scalar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f61502118d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f61469aaed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting your derivative\n",
    "vector_0 = [1,2,3]\n",
    "\n",
    "scalar_space = np.linspace(0,7)\n",
    "\n",
    "y = [compute_weird_function(x,vector_0) for x in scalar_space]\n",
    "plt.plot(scalar_space,y,label='function')\n",
    "y_der_by_scalar = [compute_der_by_scalar(x,vector_0) for x in scalar_space]\n",
    "plt.plot(scalar_space,y_der_by_scalar,label='derivative')\n",
    "plt.grid();plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Almost done - Updates\n",
    "\n",
    "* updates are a way of changing shared variables at after function call.\n",
    "\n",
    "* technically it's a dictionary {shared_variable : a recipe for new value} which is has to be provided when function is compiled\n",
    "\n",
    "That's how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Multiply shared vector by a number and save the product back into shared vector\n",
    "\n",
    "inputs = [input_scalar]\n",
    "outputs = [scalar_times_shared] #return vector times scalar\n",
    "\n",
    "my_updates = {\n",
    "    shared_vector_1:scalar_times_shared #and write this same result bach into shared_vector_1\n",
    "}\n",
    "\n",
    "compute_and_save = theano.function(inputs, outputs, updates=my_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shared value: [ 0.  1.  2.  3.  4.]\n",
      "compute_and_save(2) returns [array([ 0.,  2.,  4.,  6.,  8.])]\n",
      "new shared value: [ 0.  2.  4.  6.  8.]\n"
     ]
    }
   ],
   "source": [
    "shared_vector_1.set_value(np.arange(5))\n",
    "\n",
    "#initial shared_vector_1\n",
    "print \"initial shared value:\" ,shared_vector_1.get_value()\n",
    "\n",
    "# evaluating the function (shared_vector_1 will be changed)\n",
    "print \"compute_and_save(2) returns\",compute_and_save(2)\n",
    "\n",
    "#evaluate new shared_vector_1\n",
    "print \"new shared value:\" ,shared_vector_1.get_value()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression example\n",
    "\n",
    "Implement the regular logistic regression training algorithm\n",
    "\n",
    "Tips:\n",
    "* Weights fit in as a shared variable\n",
    "* X and y are potential inputs\n",
    "* Compile 2 functions:\n",
    " * train_function(X,y) - returns error and computes weights' new values __(through updates)__\n",
    " * predict_fun(X) - just computes probabilities (\"y\") given data\n",
    " \n",
    " \n",
    "We shall train on a two-class MNIST dataset\n",
    "* please note that target y are {0,1} and not {-1,1} as in some formulae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y [shape - (360,)]: [0 1 0 1 0 1 0 0 1 1]\n",
      "X [shape - (360, 64)]:\n",
      "[[  0.   0.   5.  13.   9.   1.   0.   0.   0.   0.  13.  15.  10.  15.\n",
      "    5.   0.   0.   3.  15.   2.   0.  11.   8.   0.   0.   4.  12.   0.\n",
      "    0.   8.   8.   0.   0.   5.   8.   0.   0.   9.   8.   0.   0.   4.\n",
      "   11.   0.   1.  12.   7.   0.   0.   2.  14.   5.  10.  12.   0.   0.\n",
      "    0.   0.   6.  13.  10.   0.   0.   0.]\n",
      " [  0.   0.   0.  12.  13.   5.   0.   0.   0.   0.   0.  11.  16.   9.\n",
      "    0.   0.   0.   0.   3.  15.  16.   6.   0.   0.   0.   7.  15.  16.\n",
      "   16.   2.   0.   0.   0.   0.   1.  16.  16.   3.   0.   0.   0.   0.\n",
      "    1.  16.  16.   6.   0.   0.   0.   0.   1.  16.  16.   6.   0.   0.\n",
      "    0.   0.   0.  11.  16.  10.   0.   0.]\n",
      " [  0.   0.   1.   9.  15.  11.   0.   0.   0.   0.  11.  16.   8.  14.\n",
      "    6.   0.   0.   2.  16.  10.   0.   9.   9.   0.   0.   1.  16.   4.\n",
      "    0.   8.   8.   0.   0.   4.  16.   4.   0.   8.   8.   0.   0.   1.\n",
      "   16.   5.   1.  11.   3.   0.   0.   0.  12.  12.  10.  10.   0.   0.\n",
      "    0.   0.   1.  10.  13.   3.   0.   0.]]\n",
      "[0 1 0 1 0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "mnist = load_digits(2)\n",
    "\n",
    "X,y = mnist.data, mnist.target\n",
    "\n",
    "\n",
    "print \"y [shape - %s]:\"%(str(y.shape)),y[:10]\n",
    "\n",
    "print \"X [shape - %s]:\"%(str(X.shape))\n",
    "print X[:3]\n",
    "print y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inputs and shareds\n",
    "shared_weights = theano.shared(np.zeros(X.shape[1] + 1, dtype='float64'))\n",
    "input_X = T.matrix(\"\", dtype=\"float64\")\n",
    "input_y = T.vector(\"\", dtype=\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_y = <predicted probabilities for input_X>\n",
    "loss = <logistic loss (scalar, mean over sample)>\n",
    "\n",
    "grad = <gradient of loss over model weights>\n",
    "\n",
    "\n",
    "\n",
    "updates = {\n",
    "    shared_weights: <new weights after gradient step>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_function = <compile function that takes X and y, returns log loss and updates weights>\n",
    "predict_function = <compile function that takes X and computes probabilities of y>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for i in range(5):\n",
    "    loss_i = train_function(X_train,y_train)\n",
    "    print \"loss at iter %i:%.4f\"%(i,loss_i)\n",
    "    print \"train auc:\",roc_auc_score(y_train,predict_function(X_train))\n",
    "    print \"test auc:\",roc_auc_score(y_test,predict_function(X_test))\n",
    "\n",
    "    \n",
    "print \"resulting weights:\"\n",
    "plt.imshow(shared_weights.get_value().reshape(8,-1))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lasagne\n",
    "* lasagne is a library for neural network building and training\n",
    "* it's a low-level library with almost seamless integration with theano\n",
    "\n",
    "For a demo we shall solve the same digit recognition problem, but at a different scale\n",
    "* images are now 28x28\n",
    "* 10 different digits\n",
    "* 50k samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz\n",
      "Downloading train-labels-idx1-ubyte.gz\n",
      "Downloading t10k-images-idx3-ubyte.gz\n",
      "Downloading t10k-labels-idx1-ubyte.gz\n",
      "(50000, 1, 28, 28) (50000,)\n"
     ]
    }
   ],
   "source": [
    "from mnist import load_dataset\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = load_dataset()\n",
    "\n",
    "print X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "\n",
    "input_X = T.tensor4(\"X\")\n",
    "\n",
    "#input dimention (None means \"Arbitrary\" and only works at  the first axes [samples])\n",
    "input_shape = [None,1,28,28]\n",
    "\n",
    "target_y = T.vector(\"target Y integer\",dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Input layer (auxilary)\n",
    "input_layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\n",
    "\n",
    "#fully connected layer, that takes input layer and applies 50 neurons to it.\n",
    "# nonlinearity here is sigmoid as in logistic regression\n",
    "# you can give a name to each layer (optional)\n",
    "dense_1 = lasagne.layers.DenseLayer(input_layer,num_units=50,\n",
    "                                   nonlinearity = lasagne.nonlinearities.sigmoid,\n",
    "                                   name = \"hidden_dense_layer\")\n",
    "\n",
    "#fully connected output layer that takes dense_1 as input and has 10 neurons (1 for each digit)\n",
    "#We use softmax nonlinearity to make probabilities add up to 1\n",
    "dense_output = lasagne.layers.DenseLayer(dense_1,num_units = 10,\n",
    "                                        nonlinearity = lasagne.nonlinearities.softmax,\n",
    "                                        name='output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#network prediction (theano-transformation)\n",
    "y_predicted = lasagne.layers.get_output(dense_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hidden_dense_layer.W, hidden_dense_layer.b, output.W, output.b]\n"
     ]
    }
   ],
   "source": [
    "#all network weights (shared variables)\n",
    "all_weights = lasagne.layers.get_all_params(dense_output)\n",
    "print all_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Than you could simply\n",
    "* define loss function manually\n",
    "* compute error gradient over all weights\n",
    "* define updates\n",
    "* But that's a whole lot of work and life's short\n",
    "  * not to mention life's too short to wait for SGD to converge\n",
    "\n",
    "Instead, we shall use Lasagne builtins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mean categorical crossentropy as a loss function - similar to logistic loss but for multiclass targets\n",
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted,target_y).mean()\n",
    "\n",
    "#prediction accuracy\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted,target_y).mean()\n",
    "\n",
    "#This function computes gradient AND composes weight updates just like you did earlier\n",
    "updates_sgd = lasagne.updates.sgd(loss, all_weights,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function that computes loss and updates weights\n",
    "train_fun = theano.function([input_X,target_y],[loss,accuracy],updates= updates_sgd)\n",
    "\n",
    "#function that just computes accuracy\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's all, now let's train it!\n",
    "* We got a lot of data, so it's recommended that you use SGD\n",
    "* So let's implement a function that splits the training sample into minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An auxilary function that returns mini-batches for neural network training\n",
    "\n",
    "#Parameters\n",
    "# X - a tensor of images with shape (many, 1, 28, 28), e.g. X_train\n",
    "# y - a vector of answers for corresponding images e.g. Y_train\n",
    "#batch_size - a single number - the intended size of each batches\n",
    "\n",
    "#What do need to implement\n",
    "# 1) Shuffle data\n",
    "# - Gotta shuffle X and y the same way not to break the correspondence between X_i and y_i\n",
    "# 3) Split data into minibatches of batch_size\n",
    "# - If data size is not a multiple of batch_size, make one last batch smaller.\n",
    "# 4) return a list (or an iterator) of pairs\n",
    "# - (подгруппа картинок, ответы из y на эту подгруппу)\n",
    "def iterate_minibatches(X, y, batchsize):\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "        \n",
    "    # Shuffle at the start of epoch\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_idx = indices[start:end]\n",
    "    \n",
    "        yield X[batch_idx], y[batch_idx]\n",
    "    \n",
    "        \n",
    "        \n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "# You feel lost and wish you stayed home tonight?\n",
    "# Go search for a similar function at\n",
    "# https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 2.013s\n",
      "  training loss (in-iteration):\t\t1.887993\n",
      "  train accuracy:\t\t56.83 %\n",
      "  validation accuracy:\t\t76.44 %\n",
      "Epoch 2 of 100 took 2.077s\n",
      "  training loss (in-iteration):\t\t1.202587\n",
      "  train accuracy:\t\t78.27 %\n",
      "  validation accuracy:\t\t83.08 %\n",
      "Epoch 3 of 100 took 1.690s\n",
      "  training loss (in-iteration):\t\t0.866884\n",
      "  train accuracy:\t\t82.64 %\n",
      "  validation accuracy:\t\t86.00 %\n",
      "Epoch 4 of 100 took 1.671s\n",
      "  training loss (in-iteration):\t\t0.700119\n",
      "  train accuracy:\t\t84.82 %\n",
      "  validation accuracy:\t\t87.60 %\n",
      "Epoch 5 of 100 took 1.767s\n",
      "  training loss (in-iteration):\t\t0.603712\n",
      "  train accuracy:\t\t86.32 %\n",
      "  validation accuracy:\t\t88.25 %\n",
      "Epoch 6 of 100 took 1.864s\n",
      "  training loss (in-iteration):\t\t0.540931\n",
      "  train accuracy:\t\t87.25 %\n",
      "  validation accuracy:\t\t88.87 %\n",
      "Epoch 7 of 100 took 1.866s\n",
      "  training loss (in-iteration):\t\t0.496710\n",
      "  train accuracy:\t\t87.95 %\n",
      "  validation accuracy:\t\t89.45 %\n",
      "Epoch 8 of 100 took 1.720s\n",
      "  training loss (in-iteration):\t\t0.463980\n",
      "  train accuracy:\t\t88.45 %\n",
      "  validation accuracy:\t\t89.84 %\n",
      "Epoch 9 of 100 took 1.666s\n",
      "  training loss (in-iteration):\t\t0.438938\n",
      "  train accuracy:\t\t88.84 %\n",
      "  validation accuracy:\t\t90.06 %\n",
      "Epoch 10 of 100 took 1.672s\n",
      "  training loss (in-iteration):\t\t0.418871\n",
      "  train accuracy:\t\t89.13 %\n",
      "  validation accuracy:\t\t90.44 %\n",
      "Epoch 11 of 100 took 1.718s\n",
      "  training loss (in-iteration):\t\t0.402492\n",
      "  train accuracy:\t\t89.43 %\n",
      "  validation accuracy:\t\t90.55 %\n",
      "Epoch 12 of 100 took 1.803s\n",
      "  training loss (in-iteration):\t\t0.388850\n",
      "  train accuracy:\t\t89.66 %\n",
      "  validation accuracy:\t\t90.72 %\n",
      "Epoch 13 of 100 took 1.764s\n",
      "  training loss (in-iteration):\t\t0.377102\n",
      "  train accuracy:\t\t89.91 %\n",
      "  validation accuracy:\t\t90.87 %\n",
      "Epoch 14 of 100 took 1.710s\n",
      "  training loss (in-iteration):\t\t0.367054\n",
      "  train accuracy:\t\t90.12 %\n",
      "  validation accuracy:\t\t91.10 %\n",
      "Epoch 15 of 100 took 1.650s\n",
      "  training loss (in-iteration):\t\t0.358221\n",
      "  train accuracy:\t\t90.23 %\n",
      "  validation accuracy:\t\t91.12 %\n",
      "Epoch 16 of 100 took 1.662s\n",
      "  training loss (in-iteration):\t\t0.350377\n",
      "  train accuracy:\t\t90.42 %\n",
      "  validation accuracy:\t\t91.31 %\n",
      "Epoch 17 of 100 took 1.657s\n",
      "  training loss (in-iteration):\t\t0.343243\n",
      "  train accuracy:\t\t90.57 %\n",
      "  validation accuracy:\t\t91.34 %\n",
      "Epoch 18 of 100 took 1.645s\n",
      "  training loss (in-iteration):\t\t0.336836\n",
      "  train accuracy:\t\t90.74 %\n",
      "  validation accuracy:\t\t91.55 %\n",
      "Epoch 19 of 100 took 1.660s\n",
      "  training loss (in-iteration):\t\t0.330921\n",
      "  train accuracy:\t\t90.84 %\n",
      "  validation accuracy:\t\t91.62 %\n",
      "Epoch 20 of 100 took 1.659s\n",
      "  training loss (in-iteration):\t\t0.325577\n",
      "  train accuracy:\t\t90.97 %\n",
      "  validation accuracy:\t\t91.82 %\n",
      "Epoch 21 of 100 took 1.658s\n",
      "  training loss (in-iteration):\t\t0.320583\n",
      "  train accuracy:\t\t91.12 %\n",
      "  validation accuracy:\t\t91.89 %\n",
      "Epoch 22 of 100 took 1.658s\n",
      "  training loss (in-iteration):\t\t0.315905\n",
      "  train accuracy:\t\t91.20 %\n",
      "  validation accuracy:\t\t92.08 %\n",
      "Epoch 23 of 100 took 1.649s\n",
      "  training loss (in-iteration):\t\t0.311451\n",
      "  train accuracy:\t\t91.35 %\n",
      "  validation accuracy:\t\t92.08 %\n",
      "Epoch 24 of 100 took 1.728s\n",
      "  training loss (in-iteration):\t\t0.307396\n",
      "  train accuracy:\t\t91.45 %\n",
      "  validation accuracy:\t\t92.22 %\n",
      "Epoch 25 of 100 took 1.702s\n",
      "  training loss (in-iteration):\t\t0.303488\n",
      "  train accuracy:\t\t91.50 %\n",
      "  validation accuracy:\t\t92.26 %\n",
      "Epoch 26 of 100 took 1.670s\n",
      "  training loss (in-iteration):\t\t0.299807\n",
      "  train accuracy:\t\t91.62 %\n",
      "  validation accuracy:\t\t92.27 %\n",
      "Epoch 27 of 100 took 1.735s\n",
      "  training loss (in-iteration):\t\t0.296288\n",
      "  train accuracy:\t\t91.71 %\n",
      "  validation accuracy:\t\t92.40 %\n",
      "Epoch 28 of 100 took 1.678s\n",
      "  training loss (in-iteration):\t\t0.292918\n",
      "  train accuracy:\t\t91.80 %\n",
      "  validation accuracy:\t\t92.45 %\n",
      "Epoch 29 of 100 took 1.682s\n",
      "  training loss (in-iteration):\t\t0.289623\n",
      "  train accuracy:\t\t91.89 %\n",
      "  validation accuracy:\t\t92.57 %\n",
      "Epoch 30 of 100 took 1.665s\n",
      "  training loss (in-iteration):\t\t0.286550\n",
      "  train accuracy:\t\t91.96 %\n",
      "  validation accuracy:\t\t92.61 %\n",
      "Epoch 31 of 100 took 1.676s\n",
      "  training loss (in-iteration):\t\t0.283631\n",
      "  train accuracy:\t\t92.04 %\n",
      "  validation accuracy:\t\t92.65 %\n",
      "Epoch 32 of 100 took 1.733s\n",
      "  training loss (in-iteration):\t\t0.280674\n",
      "  train accuracy:\t\t92.09 %\n",
      "  validation accuracy:\t\t92.69 %\n",
      "Epoch 33 of 100 took 1.661s\n",
      "  training loss (in-iteration):\t\t0.277869\n",
      "  train accuracy:\t\t92.14 %\n",
      "  validation accuracy:\t\t92.76 %\n",
      "Epoch 34 of 100 took 1.670s\n",
      "  training loss (in-iteration):\t\t0.275196\n",
      "  train accuracy:\t\t92.25 %\n",
      "  validation accuracy:\t\t92.81 %\n",
      "Epoch 35 of 100 took 1.679s\n",
      "  training loss (in-iteration):\t\t0.272489\n",
      "  train accuracy:\t\t92.31 %\n",
      "  validation accuracy:\t\t92.90 %\n",
      "Epoch 36 of 100 took 1.758s\n",
      "  training loss (in-iteration):\t\t0.269938\n",
      "  train accuracy:\t\t92.36 %\n",
      "  validation accuracy:\t\t92.89 %\n",
      "Epoch 37 of 100 took 1.656s\n",
      "  training loss (in-iteration):\t\t0.267404\n",
      "  train accuracy:\t\t92.41 %\n",
      "  validation accuracy:\t\t92.98 %\n",
      "Epoch 38 of 100 took 1.672s\n",
      "  training loss (in-iteration):\t\t0.265029\n",
      "  train accuracy:\t\t92.54 %\n",
      "  validation accuracy:\t\t93.12 %\n",
      "Epoch 39 of 100 took 1.671s\n",
      "  training loss (in-iteration):\t\t0.262668\n",
      "  train accuracy:\t\t92.55 %\n",
      "  validation accuracy:\t\t93.08 %\n",
      "Epoch 40 of 100 took 1.670s\n",
      "  training loss (in-iteration):\t\t0.260263\n",
      "  train accuracy:\t\t92.64 %\n",
      "  validation accuracy:\t\t93.20 %\n",
      "Epoch 41 of 100 took 1.680s\n",
      "  training loss (in-iteration):\t\t0.258087\n",
      "  train accuracy:\t\t92.68 %\n",
      "  validation accuracy:\t\t93.21 %\n",
      "Epoch 42 of 100 took 1.665s\n",
      "  training loss (in-iteration):\t\t0.255768\n",
      "  train accuracy:\t\t92.73 %\n",
      "  validation accuracy:\t\t93.28 %\n",
      "Epoch 43 of 100 took 1.671s\n",
      "  training loss (in-iteration):\t\t0.253646\n",
      "  train accuracy:\t\t92.80 %\n",
      "  validation accuracy:\t\t93.34 %\n",
      "Epoch 44 of 100 took 1.663s\n",
      "  training loss (in-iteration):\t\t0.251498\n",
      "  train accuracy:\t\t92.87 %\n",
      "  validation accuracy:\t\t93.31 %\n",
      "Epoch 45 of 100 took 1.643s\n",
      "  training loss (in-iteration):\t\t0.249432\n",
      "  train accuracy:\t\t92.95 %\n",
      "  validation accuracy:\t\t93.34 %\n",
      "Epoch 46 of 100 took 1.669s\n",
      "  training loss (in-iteration):\t\t0.247367\n",
      "  train accuracy:\t\t92.97 %\n",
      "  validation accuracy:\t\t93.46 %\n",
      "Epoch 47 of 100 took 1.679s\n",
      "  training loss (in-iteration):\t\t0.245328\n",
      "  train accuracy:\t\t93.05 %\n",
      "  validation accuracy:\t\t93.47 %\n",
      "Epoch 48 of 100 took 1.708s\n",
      "  training loss (in-iteration):\t\t0.243393\n",
      "  train accuracy:\t\t93.11 %\n",
      "  validation accuracy:\t\t93.56 %\n",
      "Epoch 49 of 100 took 1.732s\n",
      "  training loss (in-iteration):\t\t0.241436\n",
      "  train accuracy:\t\t93.16 %\n",
      "  validation accuracy:\t\t93.58 %\n",
      "Epoch 50 of 100 took 1.777s\n",
      "  training loss (in-iteration):\t\t0.239525\n",
      "  train accuracy:\t\t93.23 %\n",
      "  validation accuracy:\t\t93.67 %\n",
      "Epoch 51 of 100 took 1.768s\n",
      "  training loss (in-iteration):\t\t0.237646\n",
      "  train accuracy:\t\t93.24 %\n",
      "  validation accuracy:\t\t93.73 %\n",
      "Epoch 52 of 100 took 1.713s\n",
      "  training loss (in-iteration):\t\t0.235860\n",
      "  train accuracy:\t\t93.31 %\n",
      "  validation accuracy:\t\t93.81 %\n",
      "Epoch 53 of 100 took 1.768s\n",
      "  training loss (in-iteration):\t\t0.234059\n",
      "  train accuracy:\t\t93.36 %\n",
      "  validation accuracy:\t\t93.87 %\n",
      "Epoch 54 of 100 took 1.696s\n",
      "  training loss (in-iteration):\t\t0.232267\n",
      "  train accuracy:\t\t93.39 %\n",
      "  validation accuracy:\t\t93.86 %\n",
      "Epoch 55 of 100 took 1.767s\n",
      "  training loss (in-iteration):\t\t0.230518\n",
      "  train accuracy:\t\t93.47 %\n",
      "  validation accuracy:\t\t93.91 %\n",
      "Epoch 56 of 100 took 1.763s\n",
      "  training loss (in-iteration):\t\t0.228829\n",
      "  train accuracy:\t\t93.51 %\n",
      "  validation accuracy:\t\t94.04 %\n",
      "Epoch 57 of 100 took 2.071s\n",
      "  training loss (in-iteration):\t\t0.227183\n",
      "  train accuracy:\t\t93.58 %\n",
      "  validation accuracy:\t\t94.02 %\n",
      "Epoch 58 of 100 took 2.053s\n",
      "  training loss (in-iteration):\t\t0.225554\n",
      "  train accuracy:\t\t93.58 %\n",
      "  validation accuracy:\t\t94.05 %\n",
      "Epoch 59 of 100 took 2.070s\n",
      "  training loss (in-iteration):\t\t0.223859\n",
      "  train accuracy:\t\t93.66 %\n",
      "  validation accuracy:\t\t94.11 %\n",
      "Epoch 60 of 100 took 2.093s\n",
      "  training loss (in-iteration):\t\t0.222364\n",
      "  train accuracy:\t\t93.67 %\n",
      "  validation accuracy:\t\t94.13 %\n",
      "Epoch 61 of 100 took 2.090s\n",
      "  training loss (in-iteration):\t\t0.220780\n",
      "  train accuracy:\t\t93.76 %\n",
      "  validation accuracy:\t\t94.12 %\n",
      "Epoch 62 of 100 took 2.085s\n",
      "  training loss (in-iteration):\t\t0.219286\n",
      "  train accuracy:\t\t93.78 %\n",
      "  validation accuracy:\t\t94.15 %\n",
      "Epoch 63 of 100 took 2.099s\n",
      "  training loss (in-iteration):\t\t0.217735\n",
      "  train accuracy:\t\t93.83 %\n",
      "  validation accuracy:\t\t94.16 %\n",
      "Epoch 64 of 100 took 2.081s\n",
      "  training loss (in-iteration):\t\t0.216205\n",
      "  train accuracy:\t\t93.88 %\n",
      "  validation accuracy:\t\t94.27 %\n",
      "Epoch 65 of 100 took 2.120s\n",
      "  training loss (in-iteration):\t\t0.214834\n",
      "  train accuracy:\t\t93.94 %\n",
      "  validation accuracy:\t\t94.27 %\n",
      "Epoch 66 of 100 took 2.107s\n",
      "  training loss (in-iteration):\t\t0.213373\n",
      "  train accuracy:\t\t93.98 %\n",
      "  validation accuracy:\t\t94.31 %\n",
      "Epoch 67 of 100 took 2.107s\n",
      "  training loss (in-iteration):\t\t0.211927\n",
      "  train accuracy:\t\t94.00 %\n",
      "  validation accuracy:\t\t94.30 %\n",
      "Epoch 68 of 100 took 2.125s\n",
      "  training loss (in-iteration):\t\t0.210536\n",
      "  train accuracy:\t\t94.04 %\n",
      "  validation accuracy:\t\t94.44 %\n",
      "Epoch 69 of 100 took 2.118s\n",
      "  training loss (in-iteration):\t\t0.209196\n",
      "  train accuracy:\t\t94.10 %\n",
      "  validation accuracy:\t\t94.40 %\n",
      "Epoch 70 of 100 took 2.136s\n",
      "  training loss (in-iteration):\t\t0.207854\n",
      "  train accuracy:\t\t94.13 %\n",
      "  validation accuracy:\t\t94.44 %\n",
      "Epoch 71 of 100 took 2.096s\n",
      "  training loss (in-iteration):\t\t0.206520\n",
      "  train accuracy:\t\t94.19 %\n",
      "  validation accuracy:\t\t94.49 %\n",
      "Epoch 72 of 100 took 2.109s\n",
      "  training loss (in-iteration):\t\t0.205263\n",
      "  train accuracy:\t\t94.20 %\n",
      "  validation accuracy:\t\t94.49 %\n",
      "Epoch 73 of 100 took 2.122s\n",
      "  training loss (in-iteration):\t\t0.203936\n",
      "  train accuracy:\t\t94.22 %\n",
      "  validation accuracy:\t\t94.57 %\n",
      "Epoch 74 of 100 took 2.105s\n",
      "  training loss (in-iteration):\t\t0.202653\n",
      "  train accuracy:\t\t94.27 %\n",
      "  validation accuracy:\t\t94.60 %\n",
      "Epoch 75 of 100 took 2.119s\n",
      "  training loss (in-iteration):\t\t0.201453\n",
      "  train accuracy:\t\t94.28 %\n",
      "  validation accuracy:\t\t94.67 %\n",
      "Epoch 76 of 100 took 2.084s\n",
      "  training loss (in-iteration):\t\t0.200213\n",
      "  train accuracy:\t\t94.36 %\n",
      "  validation accuracy:\t\t94.66 %\n",
      "Epoch 77 of 100 took 2.105s\n",
      "  training loss (in-iteration):\t\t0.198966\n",
      "  train accuracy:\t\t94.37 %\n",
      "  validation accuracy:\t\t94.68 %\n",
      "Epoch 78 of 100 took 2.110s\n",
      "  training loss (in-iteration):\t\t0.197835\n",
      "  train accuracy:\t\t94.40 %\n",
      "  validation accuracy:\t\t94.80 %\n",
      "Epoch 79 of 100 took 2.081s\n",
      "  training loss (in-iteration):\t\t0.196653\n",
      "  train accuracy:\t\t94.44 %\n",
      "  validation accuracy:\t\t94.78 %\n",
      "Epoch 80 of 100 took 2.096s\n",
      "  training loss (in-iteration):\t\t0.195501\n",
      "  train accuracy:\t\t94.48 %\n",
      "  validation accuracy:\t\t94.82 %\n",
      "Epoch 81 of 100 took 2.097s\n",
      "  training loss (in-iteration):\t\t0.194366\n",
      "  train accuracy:\t\t94.53 %\n",
      "  validation accuracy:\t\t94.84 %\n",
      "Epoch 82 of 100 took 2.107s\n",
      "  training loss (in-iteration):\t\t0.193241\n",
      "  train accuracy:\t\t94.55 %\n",
      "  validation accuracy:\t\t94.87 %\n",
      "Epoch 83 of 100 took 2.132s\n",
      "  training loss (in-iteration):\t\t0.192078\n",
      "  train accuracy:\t\t94.55 %\n",
      "  validation accuracy:\t\t94.97 %\n",
      "Epoch 84 of 100 took 2.095s\n",
      "  training loss (in-iteration):\t\t0.191025\n",
      "  train accuracy:\t\t94.63 %\n",
      "  validation accuracy:\t\t94.95 %\n",
      "Epoch 85 of 100 took 2.090s\n",
      "  training loss (in-iteration):\t\t0.189941\n",
      "  train accuracy:\t\t94.66 %\n",
      "  validation accuracy:\t\t95.03 %\n",
      "Epoch 86 of 100 took 2.109s\n",
      "  training loss (in-iteration):\t\t0.188900\n",
      "  train accuracy:\t\t94.69 %\n",
      "  validation accuracy:\t\t95.03 %\n",
      "Epoch 87 of 100 took 2.077s\n",
      "  training loss (in-iteration):\t\t0.187806\n",
      "  train accuracy:\t\t94.72 %\n",
      "  validation accuracy:\t\t95.08 %\n",
      "Epoch 88 of 100 took 2.085s\n",
      "  training loss (in-iteration):\t\t0.186815\n",
      "  train accuracy:\t\t94.74 %\n",
      "  validation accuracy:\t\t95.15 %\n",
      "Epoch 89 of 100 took 2.086s\n",
      "  training loss (in-iteration):\t\t0.185814\n",
      "  train accuracy:\t\t94.76 %\n",
      "  validation accuracy:\t\t95.09 %\n",
      "Epoch 90 of 100 took 2.080s\n",
      "  training loss (in-iteration):\t\t0.184801\n",
      "  train accuracy:\t\t94.79 %\n",
      "  validation accuracy:\t\t95.17 %\n",
      "Epoch 91 of 100 took 2.106s\n",
      "  training loss (in-iteration):\t\t0.183731\n",
      "  train accuracy:\t\t94.83 %\n",
      "  validation accuracy:\t\t95.12 %\n",
      "Epoch 92 of 100 took 2.141s\n",
      "  training loss (in-iteration):\t\t0.182850\n",
      "  train accuracy:\t\t94.83 %\n",
      "  validation accuracy:\t\t95.17 %\n",
      "Epoch 93 of 100 took 2.097s\n",
      "  training loss (in-iteration):\t\t0.181825\n",
      "  train accuracy:\t\t94.85 %\n",
      "  validation accuracy:\t\t95.18 %\n",
      "Epoch 94 of 100 took 2.092s\n",
      "  training loss (in-iteration):\t\t0.180878\n",
      "  train accuracy:\t\t94.91 %\n",
      "  validation accuracy:\t\t95.21 %\n",
      "Epoch 95 of 100 took 2.135s\n",
      "  training loss (in-iteration):\t\t0.179925\n",
      "  train accuracy:\t\t94.94 %\n",
      "  validation accuracy:\t\t95.23 %\n",
      "Epoch 96 of 100 took 2.095s\n",
      "  training loss (in-iteration):\t\t0.179042\n",
      "  train accuracy:\t\t94.91 %\n",
      "  validation accuracy:\t\t95.28 %\n",
      "Epoch 97 of 100 took 2.179s\n",
      "  training loss (in-iteration):\t\t0.178112\n",
      "  train accuracy:\t\t95.00 %\n",
      "  validation accuracy:\t\t95.32 %\n",
      "Epoch 98 of 100 took 2.064s\n",
      "  training loss (in-iteration):\t\t0.177175\n",
      "  train accuracy:\t\t95.01 %\n",
      "  validation accuracy:\t\t95.31 %\n",
      "Epoch 99 of 100 took 2.111s\n",
      "  training loss (in-iteration):\t\t0.176267\n",
      "  train accuracy:\t\t95.00 %\n",
      "  validation accuracy:\t\t95.32 %\n",
      "Epoch 100 of 100 took 1.689s\n",
      "  training loss (in-iteration):\t\t0.175378\n",
      "  train accuracy:\t\t95.06 %\n",
      "  validation accuracy:\t\t95.32 %\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 100 #amount of passes through the data\n",
    "\n",
    "batch_size = 50 #number of samples processed at each function call\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t94.96 %\n",
      "We need more magic!\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "if test_acc / test_batches * 100 > 99:\n",
    "    print \"Achievement unlocked: 80lvl Warlock!\"\n",
    "else:\n",
    "    print \"We need more magic!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A better network\n",
    "\n",
    "\n",
    "* The quest is to create a network that gets at least 99% at test set\n",
    " * In case you tried several architectures and have a __detailed__ report - 97.5% \"is fine too\". \n",
    " \n",
    "__ There is a mini-report at the end that you will have to fill in. We recommend to read it first and fill in while you are iterating. __\n",
    " \n",
    "\n",
    "## Tips on what can be done:\n",
    "\n",
    "\n",
    "\n",
    " * Network size\n",
    "   * MOAR neurons, \n",
    "   * MOAR layers, \n",
    "   * Пх'нглуи мглв'нафх Ктулху Р'льех вгах'нагл фхтагн! \n",
    "   \n",
    "   \n",
    "   \n",
    " * Regularize to prevent overfitting\n",
    "   * Add some L2 weight norm to the loss function, theano will do the rest\n",
    "   * Can be done manually or via - http://lasagne.readthedocs.org/en/latest/modules/regularization.html\n",
    "   \n",
    "   \n",
    "   \n",
    " * Better optimization - rmsprop, nesterov_momentum, adadelta, adagrad and so on.\n",
    "   * Converge faster and sometimes reach better optima\n",
    "   * It might make sense to tweak learning rate, other learning parameters, batch size and number of epochs\n",
    "   \n",
    "   \n",
    "   \n",
    " * Dropout - to prevent overfitting\n",
    "   * `lasagne.layers.DropoutLayer(prev_layer, p=probability_to_zero_out)`\n",
    "   \n",
    "   \n",
    "   \n",
    " * Convolution layers\n",
    "   * `network = lasagne.layers.Conv2DLayer(prev_layer,`\n",
    "    `                       num_filters = n_neurons,`\n",
    "    `                        filter_size = (filter width, filter height),`\n",
    "    `                        nonlinearity = some_nonlinearity)`\n",
    "   * Warning! Training convolutional networks can take long without GPU.\n",
    "     * If you are CPU-only, we still recomment to try a simple convolutional architecture\n",
    "     * a perfect option is if you can set it up to run at nighttime and check it up at the morning.\n",
    " \n",
    " * Plenty other layers and architectures\n",
    "   * http://lasagne.readthedocs.org/en/latest/modules/layers.html\n",
    "   * batch normalization, pooling, etc\n",
    "   \n",
    "   \n",
    " * Nonlinearities in the hidden layers\n",
    "   * tanh, relu, leaky relu, etc\n",
    "   \n",
    " \n",
    " \n",
    "   \n",
    "There is a template for your solution below that you can opt to use or throw away and write it your way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mnist import load_dataset\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = load_dataset()\n",
    "\n",
    "print X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "\n",
    "input_X = T.tensor4(\"X\")\n",
    "\n",
    "#input dimention (None means \"Arbitrary\" and only works at  the first axes [samples])\n",
    "input_shape = [None,1,28,28]\n",
    "\n",
    "target_y = T.vector(\"target Y integer\",dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Input layer (auxilary)\n",
    "input_layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\n",
    "\n",
    "<student.code_neural_network_architecture()>\n",
    "\n",
    "dense_output = <your network output>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network predictions (theano-transformation)\n",
    "y_predicted = lasagne.layers.get_output(dense_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#All weights (shared-varaibles)\n",
    "# \"trainable\" flag means not to return auxilary params like batch mean (for batch normalization)\n",
    "all_weights = lasagne.layers.get_all_params(dense_output,trainable=True)\n",
    "print all_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss function\n",
    "loss = <loss function>\n",
    "\n",
    "#<optionally add regularization>\n",
    "\n",
    "accuracy = <mean accuracy score for evaluation> \n",
    "\n",
    "#weight updates\n",
    "updates = <try different update methods>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A function that accepts X and y, returns loss functions and performs weight updates\n",
    "train_fun = theano.function([input_X,target_y],[loss,accuracy],updates= updates_sgd)\n",
    "\n",
    "#A function that just computes accuracy given X and y\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#итерации обучения\n",
    "\n",
    "num_epochs = <how many times to iterate over the entire training set>\n",
    "\n",
    "batch_size = <how many samples are processed at a single function call>\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "if test_acc / test_batches * 100 > 99:\n",
    "    print \"Achievement unlocked: 80lvl Warlock!\"\n",
    "else:\n",
    "    print \"We need more magic!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report\n",
    "\n",
    "All creative approaches are highly welcome, but at the very least it would be great to mention\n",
    "* the idea;\n",
    "* brief history of tweaks and improvements;\n",
    "* what is the final architecture and why?\n",
    "* what is the training method and, again, why?\n",
    "* Any regularizations and other techniques applied and their effects;\n",
    "\n",
    "\n",
    "There is no need to write strict mathematical proofs (unless you want to).\n",
    " * \"I tried this, this and this, and the second one turned out to be better. And i just didn't like the name of that one\" - OK, but can be better\n",
    " * \"I have analized these and these articles|sources|blog posts, tried that and that to adapt them to my problem and the conclusions are such and such\" - the ideal one\n",
    " * \"I took that code that demo without understanding it, but i'll never confess that and instead i'll make up some pseudoscientific explaination\" - __not_ok__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hi, my name is `___ ___`, and here's my story\n",
    "\n",
    "A long ago in a galaxy far far away, when it was still more than an hour before deadline, i got an idea:\n",
    "\n",
    "##### I gonna build a neural network, that\n",
    "* brief text on what was\n",
    "* the original idea\n",
    "* and why it was so\n",
    "\n",
    "How could i be so naive?!\n",
    "\n",
    "##### One day, with no signs of warning,\n",
    "This thing has finally converged and\n",
    "* Some explaination about what were the results,\n",
    "* what worked and what didn't\n",
    "* most importantly - what next steps were taken, if any\n",
    "* and what were their respective outcomes\n",
    "\n",
    "##### Finally, after __  iterations, __ mugs of [tea/coffee]\n",
    "* what was the final architecture\n",
    "* as well as training method and tricks\n",
    "\n",
    "That, having wasted ____ [minutes, hours or days] of my life training, got\n",
    "\n",
    "* accuracy on training: __\n",
    "* accuracy on validation: __\n",
    "* accuracy on test: __\n",
    "\n",
    "\n",
    "[an optional afterword and mortal curses on assignment authors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
